\documentclass[a4paper,11pt]{article}

\usepackage{geometry}
\usepackage{listings}
\usepackage{mathrsfs}
\usepackage[font={small},labelfont={sf,bf}]{caption}
\usepackage{color}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{afterpage}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{bm}
\usepackage{bbm}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{hyperref} 
\usepackage{multirow}

\geometry{a4paper,top=3cm,bottom=3cm,left=2cm,right=2cm,heightrounded, bindingoffset=5mm}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{plain}
\newtheorem{theo}[definition]{Theorem}
\newtheorem{prop}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{cor}[definition]{Corollary}
\newtheorem{ex}[definition]{Example}
\theoremstyle{remark}
\newtheorem{rem}[definition]{Remark}
\newtheorem{rem*}[definition]{}

\newcommand*\mcup{\mathbin{\mathpalette\mcupinn\relax}}
\newcommand*\mcupinn[2]{\vcenter{\hbox{$\mathsurround=0pt
  \ifx\displaystyle#1\textstyle\else#1\fi\bigcup$}}}
\newcommand*\mcap{\mathbin{\mathpalette\mcapinn\relax}}
\newcommand*\mcapinn[2]{\vcenter{\hbox{$\mathsurround=0pt
  \ifx\displaystyle#1\textstyle\else#1\fi\bigcap$}}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\parr}{(}{)}
\DeclarePairedDelimiter{\parq}{[}{]}
\DeclarePairedDelimiter{\parqq}{\llbracket}{\rrbracket}
\DeclarePairedDelimiter{\bra}{\lbrace}{\rbrace}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\expval}{\mathbb{E}}
\DeclareMathOperator*{\varval}{\mathrm{Var}}

\begin{document}

\title{High Performance Computing \\ Homework assignment 2}
\author{Luca Venturi}
\maketitle

All the HW material is posted in the Github repository at the link \footnote{\href{https://github.com/luca-venturi/hpc17_cims_nyu/tree/master/hw2}{\texttt{https://github.com/luca-venturi/hpc17\_cims\_nyu/tree/master/hw2}}} .

\paragraph*{Comments to solved bugs}

The comments to the solved \texttt{omp\_bug$\bra{2,\dots,6}$.c} are reported at the beginning of the respective \texttt{omp\_solved$\bra{2,\dots,6}$.c}.

\paragraph*{Timings for 2D Dirichlet problem}

\begin{center}
\begin{tabular}{ |l|l|l|l|l| }
\hline
\multicolumn{5}{ |c| }{N = 100} \\
\hline
& & $\text{it}_\text{max}=1,000$ & $\text{it}_\text{max}=10,000$ & $\text{it}_\text{max}=100,000$ \\ \hline
\multirow{3}{*}{J} & $\text{T}_\text{my}$ & $0.0821$  & $0.765$ & $1.7879$ \\
 & $\text{T}_\text{cims}$ & $0.0808$  & $0.5581$ & $1.2537$ \\
 & $\text{Res}$ & $0.50797$  & $0.00651$ & $\text{it}_\text{max}=23391$ \\ \hline
\multirow{2}{*}{J-OMP} & $\text{T}_\text{my}$ & $0.0801$  & $0.6730$ & $1.6616$ \\
 & $\text{T}_\text{cims}$ & $0.0801$  & $0.4536$ & $1.0146$ \\ \hline
\multirow{3}{*}{GS} & $\text{T}_\text{my}$ & $0.1605$  & $1.4161$ & $1.6328$ \\
 & $\text{T}_\text{cims}$ & $0.1101$  & $0.8214$ & $0.9556$ \\
 & $\text{Res}$ & $0.31394$  & $0.00005$ & $\text{it}_\text{max}=11701$ \\ \hline
\multirow{3}{*}{GS-OMP} & $\text{T}_\text{my}$ & $0.1193$  & $0.8243$ & $0.9807$ \\
 & $\text{T}_\text{cims}$ & $0.0758$  & $0.5185$ & $0.6231$ \\
 & $\text{Res}$ & $0.44382$  & $0.00007$ & $\text{it}_\text{max}=12051$ \\ \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ |l|l|l|l|l| }
\hline
\multicolumn{5}{ |c| }{N = 1,000} \\
\hline
& & $\text{it}_\text{max}=1,000$ & $\text{it}_\text{max}=10,000$ & $\text{it}_\text{max}=100,000$ \\ \hline
\multirow{3}{*}{J} & $\text{T}_\text{my}$ & $7.3858$  & $73.676$ & $734.61$ \\
 & $\text{T}_\text{cims}$ & $5.2983$  & $52.492$ & $529$\\
 & $\text{Res}$ & $0.95075$  & $0.84149$ & $0.49692$ \\ \hline
\multirow{2}{*}{J-OMP} & $\text{T}_\text{my}$ & $7.0621$  & $72.813$ & $700.31$ \\
 & $\text{T}_\text{cims}$ & $4.4626$  & $51.195$ & $449.86$ \\ \hline
\multirow{3}{*}{GS} & $\text{T}_\text{my}$ & $14.388$  & $143.95$ & $1439.4$ \\
 & $\text{T}_\text{cims}$ & $8.1377$  & $81.022$ & $814.13$ \\
 & $\text{Res}$ & $0.92995$  & $0.77542$ & $0.30304$ \\ \hline
\multirow{3}{*}{GS-OMP} & $\text{T}_\text{my}$ & $7.5757$  & $75.447$ & $759.16$ \\
 & $\text{T}_\text{cims}$ & $4.8405$  & $47.78$ & $549.86$ \\
 & $\text{Res}$ & $1.31516$  & $1.09662$ & $0.42856$ \\ \hline
\end{tabular}
\end{center}

The above tables show the timings and the residuals obtained using the various method: Jacobi (J), parallel Jacobi (J-OMP), Gauss-Seidel (GS), parallel Red-Black Gauss-Seidel (GS-OMP). We ran the programs for $N=100$, $1,000$ and for different numbers of maximum iterations $\text{it}_\text{max} = 1,000$, $10,000$, $100,000$. When the residual hits the tolerance $\mathrm{tol} = 10^{-5}$ before  $\text{it}_\text{max}$, the maximum iteration used is reported. Note that the residuals for the parallel Jacobi method are not reported since they are the same of the Jacobi method. The timings reported are those obtained with my machine ($\text{T}_\text{my}$) and with a CIMS machine ($\text{T}_\text{cims}$). My machine (PC) has a processor \texttt{Intel-Core i3-4010U CPU @ 1.70 GHz} with 2 cores and a 4 GB total RAM. OS is Linux Ubuntu 16.04 (64-bit). The number of threads used in the parallel programs was $4$ for both the machines. \\ 
By the above results we can deduce the following facts:
\begin{itemize}
\item With the respect to J, J-OMP algorithm shows to be faster for big values of $N$ and $\text{it}_\text{max}$.
\item With the respect to GS, GS-OMP algorithm shows to be faster for big values of $N$ and $\text{it}_\text{max}$. Nevertheless, GS-OMP algorithm converges slower than GS (actually, for small values of $\text{it}_\text{max}$, the residual increases before starting decreasing). If, instead of $\text{it}_\text{max}$, we use $\mathrm{tol}$ as stopping criteria, the GS-OMP algorithm shows to perform better (i.e. it's faster) than the GS (at least for big enough values of $N$).
\end{itemize}
The performances of the various algorithms reflect what was expected: for both J and GS, the parallel version is faster, even if the time difference is significant only for big values of $N$ and $\text{it}_\text{max}$. Moreover we can see that the parallel Red-Black GS shows a slower convergence of the residual with respect to the standard GS, while the former is faster than the latter for a given number of iterations. This is due to the fact that Red-Black GS corresponds to a GS after a re-ordering of the variables (which translates as a preconditioning of the matrix in the linear system we are considering). Also, parallel GS can implemented using more than 2 colors: in this case, it is possible to show that the fewer the number the slower the convergence is; nevertheless the fewer the number the more parallel the algorithm is. Hence, for parallel GS there is trade-off between parallelism and efficiency.

\paragraph*{Note on the implementation of 2D parallel Red-Black Gauss-Seidel algorithm}

My implementation of the 2D parallel Red-Black Gauss-Seidel algorithm seems quite complicate; the main difficulty was to assign the right labeling to every point of the grid (depending on black or red coloring) in the updating of GS method and in the evaluation of the residual, avoiding the updating of ghost points. An additional complication was due to the fact that this labeling is dependent on $N$ even or odd. In particular this implies that my algorithm is not easily adaptable to the case of number of colors greater than 2.

\end{document}